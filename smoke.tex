%% Version 6.1, 1 September 2021
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% amspaperV6.tex --  LaTeX-based instructional template paper for submissions to the 
% American Meteorological Society
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Start with one of the following:
% 1.5-SPACED VERSION FOR SUBMISSION TO THE AMS
\documentclass{ametsocV6.1}

% TWO-COLUMN JOURNAL PAGE LAYOUT---FOR AUTHOR USE ONLY
% \documentclass[twocol]{ametsocV6.1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% To be entered by author:

%% May use \\ to break lines in title:

\title{Using Deep Learning to Develop an Annotated Dataset of Smoke Plumes in Satellite Imagery}

%% Enter authors' names and affiliations as you see in the examples below.
%
%% Use \correspondingauthor{} and \thanks{} (\thanks command to be used for affiliations footnotes, 
%% such as current affiliation, additional affiliation, deceased, co-first authors, etc.)
%% immediately following the appropriate author.
%
%% Note that the \correspondingauthor{} command is NECESSARY.
%% The \thanks{} commands are OPTIONAL.
%
%% Enter affiliations within the \affiliation{} field. Use \aff{#} to indicate the affiliation letter at both the
%% affiliation and at each author's name. Use \\ to insert line breaks to place each affiliation on its own line.

\authors{Rey Koki,\aff{a, b, c}\correspondingauthor{Rey Koki, rey.koki@noaa.gov}
Micheal McCabe,\aff{a} 
Jebb Q. Stewart,\aff{b} 
Christina Kumler,\aff{b, c} 
Jed Brown,\aff{a} 
Dhruv Kedar,\aff{d} 
Wilfrid Schroeder \aff{e}
}
\affiliation{\aff{a}{Computer Science Department, University of Colorado, Boulder, Colorado}\\
\aff{b}{NOAA's Global Systems Laboratory, Boulder, Colorado}\\
\aff{c}{CIRES, University of Colorado, Boulder, Colorado}\\
\aff{d}{JILA and Department of Physics, University of Colorado, Boulder, Colorado}\\
\aff{e}{NOAA NESDIS, College Park, Maryland}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%
% Enter your abstract here
% Abstracts should not exceed 250 words in length!
%

\abstract{The increase in the frequency of wildfires on a global scale underscores the need for advancements in monitoring techniques for effective health related mitigation, disaster management and environmental protection. This research introduces an innovative, data-driven framework for creating annotated geostationary satellite imagery, specifically focused the detection of smoke plumes by leveraging the data generated by NOAA's (National Oceanic and Atmospheric Administration) Hazard Mapping Systems (HMS). The primary objective is to refine the existing HMS dataset, that provides temporal and geographical information on individual smoke plumes, and then use deep learning to pinpoint the singular, most representative, GOES image that optimally illustrates the smoke annotation within the given time window. By identifying the most representative imagery of smoke plumes within the given time window, the study seeks to create a highly precise and relevant training dataset. The resulting dataset is anticipated to be an instrumental tool in developing further machine learning models, such as an automated system capable of real-time monitoring and annotation of smoke plumes directly from satellite imagery.
    }
\begin{document}


\maketitle

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COMMENT OUT THIS LINE TO COMPILE IMAGES
\setkeys{Gin}{draft}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In recent years, the escalation of wildfire incidents worldwide has become a prominent environmental and public health concern. The combustion process in wildfires releases vast amounts of smoke containing fine particulate matter (PM2.5) and harmful gases, posing severe risks to human health and air quality. This scenario underscores the necessity for efficient and effective monitoring methods to mitigate the adverse health impacts associated with wildfire smoke. 

Traditionally, wildfire monitoring has relied on ground-based methods, such as forest service patrols, manned lookout towers, and aviation surveillance. While these methods provide valuable local insights, they are constrained by geographical and logistical limitations, often failing to deliver timely and comprehensive data, especially over large and remote areas. In contrast, satellite imagery offers a vantage point that overcomes these limitations, providing continuous, wide-area coverage and real-time data crucial for assessing and responding to the health risks posed by wildfire smoke.

Satellite imagery, equipped with advanced sensors, such as the Advanced Baseline Imager on the Geostationary Operational Environmental Satellites (GOES), have revolutionized environmental monitoring. These tools enable the detailed observation of smoke plumes, their particulate density, and the extent of smoke spread. These satellite-based systems offer the capabilities to provide critical insights into the concentration and movement of smoke particulates, facilitating accurate and timely assessments of air quality.

The integration of satellite imagery in wildfire smoke monitoring is not only instrumental in providing real-time data but also plays a significant role in public health planning and response. By mapping the spread and density of smoke, health authorities can issue timely warnings, implement evacuation protocols, and deploy resources effectively to mitigate health risks. Furthermore, long-term data gathered from satellite observations can aid in understanding the broader impacts of wildfire smoke on public health, influencing policy decisions and preventive measures.

Currently, multi-channel thresholding is a popular method to distinguish smoke pixels from pixels containing dust, clouds or other phenomenon with similar signatures. The method uses historical, labeled data to extract optimal radiance values for each channel that corresponds with the labeled class. These methods are tuned to particular biogeographies and often have issues with generalization to new locations with varying fuel types \citep{thresh_geog}.

In contrast to the numerical thresholding approach, human visual inspection of satellite imagery is another commonly used method for smoke identification. Trained analyst will inspect imagery and label the smoke by hand. This method is not as extendable as one that is automated and is limited by the availability of analyst and their time.


To address these challenges we can look towards innovative approaches and technological advancements in computer vision. Machine learning methods have shown promising potential in improving the accuracy and efficiency of satellite-based wildfire smoke detection and monitoring. For instance, SmokeNet, uses convolutional neural network (CNN) based framework to determine if a scene of MODIS imagery contains smoke \citep{smokenet}. Another study also used a CNN to identify smoke on a pixel-wise basis
using imagery from Himiwari-8 \citep{larsen}. Additionally, Wen et al. developed a CNN architecture that takes GOES-EAST imagery as input and HMS annotations for the target labels during training\citep{smoke_goes}. 



The success of deep learning methods, such as CNNs, relies heavily on the availability of a large, representative dataset \citep{data_size}. Existing methods use relatively small amounts of data, from 57 \citep{wang} to 6825 \citep{smoke_goes}. In contrast, benchmark datasets for image classification contain tens of thosands (CIFAR-10 and MNIST) to millions (CIFAR-100 and ImageNet) of data samples. Keeping in mind the correlation between both the quality and quantity of data with model performance, we
introduce the largest known smoke dataset containing over 100,000 independent samples.

\begin{table}[h]
\caption{Comparison of different studies including method used, dataset size, satellite source, number of channels used and if the detection is done at a pixel or image level.}\label{studies}
\begin{center}
\begin{tabular}{ccccrrcrc}
\topline
Reference & Method & Number of Images & Satellite & \verb|#| Channels & Detection Level\\
\midline
\citep{smoke_goes}& CNN & & GOES-EAST & 6825 & pixel\\
\citep{smokenet}& CNN & 6255 & MODIS & 5 & image\\
\cite {larsen} & CNN & 975 & Himiwari-8 & 7 & pixel\\
\citep{wang}& U-Net & 47 & Landsat-8 & 13 & pixel\\
SmokeViz (2023) & U-Net & 60,000 & GOES-EAST, GOES-WEST & 3 & pixel\\
\botline
\end{tabular}
\end{center}
\end{table}

\section{Methods}
\subsection*{Dataset}

Dataset development came in three stages. First, we developed a rough dataset of satellite imagery by accounting for light scattering to make an educated guess on which image would best represent the analyst annotation. Second, we used that dataset to train a model that would identify smoke in satellite imagery. Third, we use that model to determine the optimal satellite image to represent each annotation. 

\subsubsection*{Smoke Labels} 
The National Environmental Satellite, Data and Information Service (NESDIS) and National Oceanic and Atmospheric Administration (NOAA) manage environmental satellite programs such as the Hazard Mapping System (HMS) \citep{hms, hms_val}.  The HMS program is an operational system that uses an aggregation of satellite data to generate active fire and smoke data that is used in applications such as air quality assessments and serves as verification and validation for NOAA's smoke forecasting model, HYSPLiT, \citep{hysplit_ver}. To train our model, we implement a supervised learning framework that uses the HMS analyst smoke product as truth labels during the model training process.

HMS smoke analysis data gives the coordinates of the smoke perimeter and classifies the smoke by density within a given time window. The time windows can range from instantaneous (same start/end time) to lengths of 5 hours. While the bounds of the smoke annotations can change within the larger time spans, the analyst is making an approximation that should reflect the smoke coverage over the duration of the window. The density information is qualitatively determined by the analyst based on smoke opacity and categorized as either light, medium or heavy as seen in figure \ref{densities}a.

\begin{figure}
    \centering
    \includegraphics[width=15cm]{figures/Misclassified.png}
    \caption{Satellite imagery captured by GOES-EAST within a few days of each other. The yellow, orange and red contours indicate the extent of Light, Medium and Heavy smoke.  a) shows a canonical example of a smoke plume. b) and c) show variations in the qualitatively determined density labels. b) we show Medium and Heavy densities of smoke that, upon visual inspection, could be interpreted as less dense than portions of the Light density smoke labeled in c).}\label{densities}
\end{figure}


\subsubsection*{Thermometer Encoding Smoke Densities}

One of the challenges introduced with using human generated qualitative smoke densities was that, as seen in figure \ref{densities}b and \ref{densities}c, there are variations in what is labeled as heavy or light density smoke. More generally, reproducing qualitative metrics with quantitative algorithms is a challenging problem, but we apply mathematical approaches that mitigate some of the underlying complications of our specific problem. Despite the fact that the smoke densities introduce qualitative complexities, we decided that the density approximations were important to use in our dataset because of the differences in signatures the densities produce. Within the satellite imagery, the appearance of a light density smoke plume will look significantly different than a heavy density smoke plume as seen in figure \ref{densities}. Additionally, a light density smoke plume is expected to be more challenging to detect since it is easier for it to be misclassified as not smoke. During the training process, the separate density categories allows us to deferentially weight the penalization given to the model for incorrect classifications based on category. For example, the model can be given a small penalization for misclassifying light smoke as not smoke while given a higher penalization for misclassifying heavy smoke as not smoke. 

In addition to the densities being ordered and categorical, the differences in opacity is undefined by any metric such as particulates per square meter, so we can treat the density labels as ordinal instead of just categorical data. This allows us to use thermometer encoding, which leverages the idea that heavy density smoke includes both medium and light density smoke, that heavy density smoke is closer to medium than it is to light and automatically weights the loss functions and incorporates
the ranked ordering of the densities.  As seen in Table \ref{therm}, one-hot encoding, commonly used for categorical data, doesn't take ordinal properties of the data into consideration.  



\begin{table}[h] 
\caption{A comparison of one-hot encoding used for categorical data to thermometer encoding for ordinal data.}\label{therm}
\begin{center}
\begin{tabular}{ccccrrcrc}
\topline
category & one-hot & thermometer \\
\midline
No Smoke & \texttt{[0 0 0]} & \texttt{[0 0 0]} \\
Light  & \texttt{[0 0 1]} & \texttt{[0 0 1]} \\
 Medium & \texttt{[0 1 0]} & \texttt{[0 1 1]} \\
 Heavy  & \texttt{[1 0 0]} & \texttt{[1 1 1]} \\
\botline
\end{tabular}
\end{center}
\end{table}

\subsubsection*{Time Windows For Smoke Annotations}

In order to take into account movement characteristics to help identify smoke, analysts use multi-frame animations of the satellite imagery. The resulting annotations often have large time windows over multiple hours to represent one smoke plume. Since their goal is to show the general coverage over that time span, often the smoke boundaries don't match up with the satellite imagery over the entire time window \ref{timelapse}. One way to approach this problem would be to use all the satellite images the
analysts used as input. Since the timespans are non-uniform, this would vary the length in imagery inputs into the model, which would be difficult with a CNN architecture. Moreover, this would require a large amount of additional memory and computational resources. Instead of using the original analysts' many satellite image inputs to one annotated output, we decide to develop a one-to-one input-to-output by finding the best singular satellite image input to represent the annotation. As
discussed in the next section, we do this by making physics based choices on which satellite and timestamp would give the optimal angle between the sun and satellite that would produce the strongest smoke signature for the geolocation and timestamp of the smoke plume. 

\begin{figure} \label{timelapse}
    \centering
    \includegraphics[width=16cm]{figures/timelapse.png}
    \caption{True Color GOES imagery from May 2022, Southeast New Mexico (31$^{\circ}$ Latitude, -100$^{\circ}$ Longitude) during the start of the Foster Fire. The HMS annotations for the smoke outlines shown here spanned from 19:10-23:00 UTC but visually match the smoke location for the last part of the }
\end{figure}

\subsubsection*{Satellite Imagery} 

The Geostationary Operational Environmental Satellites (GOES) are operated by the NOAA and NESDIS support meteorology research and forecasting for the United States. We use the latest operational satellites, GOES-16 (EAST), 17 and 18 (WEST) that carry the Advanced Baseline Imager (ABI), that measure 16 bands between the visible and infrared wavelengths. In improvement to the GOES predecessors, imagery is collected every 5 minutes for the contiguous United States and every 10 minutes for the full disk. We use bands 1-3 (Table \ref{rgb_bands}) as input to Satpy's composite algorithm to develop a true color image representation, similar to what is used as input by HMS analysts \citep{satpy} and \citep{true_color}.


\begin{table}[h]
\caption{To create a true color image, we use the following bands from the Advanced Baseline Imager Level 1b CONUS (ABI-L1b-RadC) product.}\label{rgb_bands}
\begin{center}
\begin{tabular}{ccccrrcrc}
\topline
band & description & center wavelength & spatial resolution (km)\\
\midline
C01 &  blue visible & 0.47 & 1 \\
C02 & red visible & 0.64 & 0.5 \\
C03 & veggie near infrared & 0.865 & 1 \\
\botline
\end{tabular}
\end{center}
\end{table}

We used a physics-informed approach in selecting the initial dataset for training our model. Rather than use the cumulative data from GOES-WEST and GOES-EAST images, we select one or the other based on the solar zenith angle. For smoke identification, such an approach can achieve a much higher signal-to-noise than imaging the earth’s surface from an arbitrary angle. The elastic scattering of light is key to this approach - while the atmosphere is composed of molecules with size $<1$ nm, smoke particles can vary from 100 nm - 10 $\mu$m in diameter, $d$. The GOES ABI covers spectral bands from 0.47 $\mu$m - 13.3 $\mu$m, so atmospheric and smoke particle sizes occupy two very different regimes with respect to the imaging wavelength $\lambda$, as shown in figure \ref{regime}. In the extreme limit of $\lambda \gg d$, the physics of scattering of light off a small sphere is captured by Rayleigh scattering. This process has two critical consequences: (1) the scattering cross section of light is strongly wavelength dependent (scaling with $\lambda^{-4}$), meaning that photons with wavelength closer to the ultraviolet are scattered more strongly than infrared photons. (2) the scattering cross section scales with an angular dependent cross section of $(1 + \cos^2 \theta)$. Scattered photons follow the emission distribution of a radiating dipole, scattering more strongly in the forward and backwards directions $(\theta = 0,\pi)$ than orthogonal to the direction of propagation $(\theta = \pi/2, 3\pi/2)$, see figure \ref{mei} for Rayleigh scattering schematic.

\begin{figure}
    \centering
    \includegraphics[width=10cm]{figures/scatter_regime.png}
    \caption{Relationship between the size of a particle, the wavelength of light interacting with the particle and the type of scattering behavior induced by that interaction. The dotted lines represent rough estimates of the boundaries between the scattering regimes \citep{petty}. The gray area represents the range of particle radius relevant to smoke particulate matter.}\label{regime}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=10cm]{figures/mei.png}
    \caption{If the particle size is $<\frac{1}{10}$ the wavelength of the interacting light, then the primary scattering will be Rayleigh. Mie scattering is the predominant scattering mechanism when the particle size is larger than wavelength of light.} \label{mei}
\end{figure}

The significance of these scalings is that the observer, or detector, will receive blue photons in most directions orthogonal to the source. Equivalently, photons traveling colinear with line of sight to the emission source will mostly have wavelengths in the infrared band. In the converse regime of $d \gtrsim \lambda$, the elastic scattering of light against matter is modeled through Mie scattering. Unlike Rayleigh scattering, Mie scattering is largely wavelength independent and has a more complicated radiation pattern where the cross section has a maximal amplitude in the forward direction. An observer downstream of this scatterer will collect more photons than one positioned directly behind it. In the context of smoke identification, a sunrise or sunset will lead to a higher Mie scattered signal in GOES-WEST and GOES-EAST respectively, as shown with a smoke plume producing a stronger signal in GOES-EAST imagery near sunset in figure \ref{16_vs_17}.

\begin{figure}\label{16_vs_17}
    \centering
    \includegraphics[width=15cm]{figures/G16_v_G17.png}
    \caption{True Color GOES-EAST (left) and GOES-WEST (right) imagery from April $24^{th}$, 2022. The images were taken about 1.5 hours before sunset for this geolocation and time of year (01:43 UTC).}\label{16vs17}
\end{figure}

Smoke identification therefore amounts to extracting a signal of $d \gtrsim \lambda$ photons from the $\lambda \gg d$ background. Positioning a detector along line of sight to the scatterer will result in a higher signal from smoke particles (figure \ref{mei}). Filtering the imaged wavelength can enhance this signal; photons collected in the blue spectrum will have a naturally lower background along the line of sight to the illumination source do their high level of Rayleigh scattering as. Therefore, as demonstrated in figure \ref{bands}, this configuration results in the highest signal to noise imaging for smoke particles. 



\begin{figure}
    \centering
    \includegraphics[width=16cm]{figures/GOES16_bands.png}
    \caption{The three bands of GOES-EAST data are the raw input to generate the True Color image shown in figure \ref{16vs17}. There appears to be a higher signal-to-noise ratio for smoke detection as the wavelength, $\lambda$, of light being measured decreases.}\label{bands}
\end{figure}


After these realizations, we concluded that the optimal times were to grab data from GOES-WEST right after sunrise and from GOES-EAST right before sunset. Another consideration we needed to account for was that when the sun is in optimal alignment with the satellite for detecting smoke also coincides with the maximal amount of atmosphere the light travels through. This is shown in figure \ref{G17_sunrise}, where the noise introduced by higher amounts of atmospheric noise can make the signal from the smoke difficult to discern despite the smoke signal being at its highest. This phenomenon is even more prominent the further the smoke is longitudinally from the satellite since there will be more atmosphere to travel through between the light interacting with the smoke and reaching the detector. Additionally shown in figure \ref{G17_sunrise}, and is especially evident for data close to sunrise, when the time window is large, the smoke has often not dispersed to the extent of the analysts' annotation boundaries. We consider the atmospheric interaction noise in our algorithms to develop the dataset by choosing a lag time between sunrise and optimal image timestamp as a function of longitude, but that only served as an approximation that improved our dataset but still produced poor quality data in certain circumstances. There were additional variables, such as latitude and time of year that were becoming unwieldy to the point that we decided to consider alternative options since even if we were able to pinpoint an algorithm that accounted for the atmospheric noise we would not be able to account for the smoke's alignment to the annotation.  Instead, as we will discuss in further detail, we use the current dataset to train a machine learning model that would then be able to go back to the original satellite imagery options and choose which image best overlaps with the analyst annotation.

\begin{figure}
    \centering
    \includegraphics[width=16cm]{figures/timelapse_G17_2.png}
    \caption{A smoke annotation projected onto GOES-WEST imagery from August 2022 that spans from 11:00 UTC to 15:00 UTC, sunrise on August 2nd, 2022 at coordinates (49°24'N, 115°29'W) was 12:15 UTC. 15 minutes after the calculated sunrise, the imagery contains a noticeably higher levels of noise than the subsequent imagery thats' light travels through (thus interacts with) less atmosphere as the sun rises and the angle between GOES-WEST and the sun decreases. }\label{G17_sunrise}
\end{figure}




\subsubsection*{Machine Learning Model} 

We implement a deep learning architecture that uses the encoder from the ResNet model \cite{resnet} and a semantic segmentation classifier from the U-Net model \cite{unet}. Transfer learning has shown to reduce the time and resources needed to train a model by leveraging information from pre-trained models \cite{transfer}, \cite{transfer2}.  We initialize the values of our model weights using the pre-trained values originally trained on the ImageNet dataset \cite{imgnet}, containing 1.2 million images and 1000 categories. Our model was developed using the Segmentation Models PyTorch package \cite{semantic} that was written as a high level API for implementing models for semantic segmentation problems.  We input 256x256x3 snapshots of True Color GOES imagery that contains smoke and output a 256x256x3 classification map that predicts if a pixel contains smoke and if so, what the density of that smoke is. As mentioned earlier, we apply the thermometer encoding shown in table \ref{therm} to encode the smoke densities and apply binary cross entropy as the loss function per density of smoke. 

The original dataset developed using the Mie algorithm contained over 120,000 samples. To train our model, we split the dataset into training (95,000 samples), validation (12,000 samples) and testing (12,000) datasets. Training data contains data from the years 2018, 2019, 2020, 2021 and 2023 while the data from 2022 is split into validation and testing data by taking data from alternating weeks of the year. Splitting 2022 data by week allowed us to leave more full years of data for the training set and allowed each dataset to show yearlong trends while trying to keep the datasets independent from one another.

We trained a model over 20 epochs and then used that model to develop the dataset by determining which satellite image provided the best Intersection over Union (IoU) value. The IoU metric is given by the ratio of area of overlap to the area of union as defined in equation \ref{iou}, where A and B are the truth labels and the model's predictions.

\begin{equation} \label{iou}
    IoU = \frac{| A \cap B|}{|A|\cup|B|}
\end{equation}

To determine which image best represents the analyst annotation, we gather all the satellite imagery for the given time window and run them through the machine learning model. The output of the model gives a prediction on if there is smoke in the image, if so, where the smoke is in that image and what the density of that smoke is. For each density predicted, we calculate the IoU using the total set of pixels that the model predicts as that density of smoke and the entire set of pixels labeled by the analyst as a particular smoke density for each image. The image with the highest IoU score is chosen as the image that best represents the analyst smoke annotation. If the maximum IoU value was under .1, we did not include that annotation in the dataset.. 



\section*{Results}

To interpret the performance of our trained model, we report the IoU metrics in table \ref{iou_results} that were computed by running the model on the Mie algorithm derived dataset and the ML derived Dataset. For each density, we calculate the IoU using the total set of pixels that the model predicts as that density of smoke and the entire set of pixels labeled by the analyst as a particular smoke density over all imagery contained in the testing dataset. Additionally, we compute the overall IoU for all densities by first computing the number of pixels that insect their correct density and divide that by the total number of pixels that make up the union of model predicted and analyst labeled smoke as shown in equation \ref{overall_iou}.


\begin{equation} \label{overall_iou}
    IoU_{overall} = \frac{\sum\limits_{i=light}^{heavy}|A_{i}\cap B_{i}|  }{\sum\limits_{i=light}^{heavy}|A_{i}|\cup|B_{i}|}
\end{equation}

\begin{table}[h] 
\caption{IoU results per density of smoke and over all densities.}\label{iou_results}
\begin{center}
\begin{tabular}{ccccrrcrc}
\topline
category & IoU Mie Dataset &IoU ML Dataset \\ 
\midline
Light  & 0.394 &  0.551 \\
Medium & 0.283 &  0.392 \\
Heavy  & 0.233 &  0.290 \\
Overall & 0.365 &  0.510 \\
\botline
\end{tabular}
\end{center}
\end{table}



\section{Conclusion}


In this study, we have refined an existing dataset originally curated by the HMS team, transforming it from a many-to-one imagery-to-annotation format to a more concise one-to-one satellite image-to-annotation dataset. The initial HMS dataset primarily gave a approximation of where smoke had been present for a given time window, though it did not confirm the actual existence of smoke in the pixels of the selected images. Due to that nature of the HMS dataset, our Mie derived dataset gave an approximation of when we'd best be able to measure the smoke signal but did not factor in if the smoke was actually present in the selected image. This discrepancy can be detrimental when training a machine learning model, as it may penalize accurate predictions and inadvertently introduce biases towards misclassifying noise as meaningful signal. 

To make improvements on the dataset's reliability, we apply a machine learning model trained on the Mie-derived dataset to select the most appropriate satellite images within the timeframe specified by the analyst's annotation. A notable illustration of the improvements introduced by the machine learning method is evident in Figure \ref{ml_vs_mei}. The annotation associated with this example encompasses five hours of imagery and we show the images that is chosen by each of our two methods. While the Mie algorithm tries to optimize for the highest possible signal-to-noise, which is the image closest to sunrise, our machine learning algorithm chooses the image that maximizes the overlap of smoke predicted by the model with the analyst's annotation.



\begin{figure}
    \centering
    \includegraphics[width=15cm]{figures/ML_better_than_Mei_2.png}
    \caption{GOES-WEST imagery showing smoke on June 8th, 2022 in Alaska where at the coordinates (61°03'N, 156°07'W), daylight was between 12:43-7:53 UTC. The smoke annotations displayed span from 18:50 to 23:50 UTC. a) shows the imagery that was selected using the Mie algorithm, which optimizes for the image closest to sunrise. b) shows the imagery chosen by the ML algorithm that had the highest IoU score. The IoU scores are similar for the low and medium density smoke, but the
    high density smoke IoU for a) is .01 while b) is significantly higher at .59.}\label{ml_vs_mei}
\end{figure}


The result of this study is a curated dataset that can be used to train machine learning models for various wildfire smoke applications. The end goal is to produce a robust and reliable machine learning based approach for detecting wildfires using satellite imagery. That information can be used for wildfire monitoring and as data provided to public health officials for air quality assessments.

%\begin{figure} \label{results}
%    \centering
%    \includegraphics[width=16cm]{figures/results.png}
%    \caption{}
%\end{figure}


\clearpage

\acknowledgments

\datastatement





\bibliographystyle{ametsocV6}
\bibliography{references}


\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END OF AMSPAPERV6.1.TEX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
